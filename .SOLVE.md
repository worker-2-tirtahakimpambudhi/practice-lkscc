Error yang kamu alami ini:

```
Internal error occurred: failed calling webhook "mservice.elbv2.k8s.aws": failed to call webhook: Post "https://aws-load-balancer-webhook-service.kube-system.svc:443/...": no endpoints available for service "aws-load-balancer-webhook-service"
```

menunjukkan bahwa Kubernetes tidak bisa mengakses webhook dari **AWS Load Balancer Controller**, tepatnya `aws-load-balancer-webhook-service` karena **tidak ada endpoint aktif**.

### Penyebab Umum
Masalah ini sering terjadi karena:
1. **AWS Load Balancer Controller belum ter-deploy atau crash.**
2. **Namespace `kube-system` tidak memiliki pod yang menjalankan service `aws-load-balancer-webhook-service`.**
3. **Webhook service tidak terdaftar dengan benar.**
4. **Webhook deployment belum siap ketika kamu apply manifest.**

---

### Cara Mengatasi

#### ‚úÖ 1. Cek apakah controller sudah running
```bash
kubectl get pods -n kube-system | grep aws-load-balancer
```

Kamu harus melihat pod seperti:
```
aws-load-balancer-controller-xxxxxx   Running
```

Jika tidak ada atau `CrashLoopBackOff`, kamu perlu deploy ulang.

#### ‚úÖ 2. Cek service dan endpoint
```bash
kubectl get svc -n kube-system | grep aws-load-balancer
kubectl get endpoints aws-load-balancer-webhook-service -n kube-system
```

Kalau endpoints kosong (`<none>`), berarti pod yang harusnya serve webhook belum ready.

#### ‚úÖ 3. Redeploy AWS Load Balancer Controller
Kalau belum ter-install atau rusak, kamu bisa deploy ulang pakai Helm:

```bash
helm repo add eks https://aws.github.io/eks-charts
helm repo update

helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
  -n kube-system \
  --set clusterName=<YOUR_CLUSTER_NAME> \
  --set serviceAccount.create=false \
  --set serviceAccount.name=aws-load-balancer-controller \
  --set region=<YOUR_REGION> \
  --set vpcId=<YOUR_VPC_ID>
```

Jangan lupa ganti:
- `<YOUR_CLUSTER_NAME>`
- `<YOUR_REGION>`
- `<YOUR_VPC_ID>`

> Pastikan juga service account `aws-load-balancer-controller` sudah di-attach IAM role-nya (jika kamu pakai EKS IRSA).

---

### Catatan Penting:
- Kamu bisa **sementara waktu menghapus bagian Ingress** dari file YAML untuk melanjutkan testing pod dan service.
- Setelah AWS Load Balancer Controller siap, baru apply kembali bagian Ingress-nya.

---

Kalau kamu pakai EKS, kasih tahu juga apakah kamu deploy pakai Helm atau eksctl/terraform, biar aku bisa bantu lebih presisi. Mau dibantu cek step-by-step deploy ulang ALB Controller juga boleh.

Nah ini dia akar masalahnya: node kamu statusnya **`SchedulingDisabled`**, artinya Kubernetes **tidak bisa menjadwalkan pod apapun ke node tersebut**. Biasanya ini karena node kamu **masih dalam proses join**, **drain**, atau belum **fully ready**.

---

## ‚úÖ Cara Memperbaiki Node `SchedulingDisabled` di EKS (eksctl)

### üîß Opsi 1: Enable Scheduling Lagi

Kalau node kamu seharusnya sudah ready, kamu bisa enable kembali scheduling dengan:

```bash
kubectl uncordon ip-192-168-38-162.us-east-2.compute.internal
```

> `uncordon` artinya mengizinkan kembali penjadwalan pod ke node tersebut.

Setelah itu coba cek ulang status node:

```bash
kubectl get nodes
```

Pastikan jadi `Ready` saja, tanpa `SchedulingDisabled`.

---

### üîß Opsi 2: Buat Node Group Baru (via `eksctl`)

Kalau node kamu rusak/gagal setup, kamu bisa buat node group baru via `eksctl`:

```bash
eksctl create nodegroup \
  --cluster <CLUSTER_NAME> \
  --name standard-workers \
  --node-type t3.medium \
  --nodes 2 \
  --nodes-min 1 \
  --nodes-max 3 \
  --managed
```

> Ganti `<CLUSTER_NAME>` dengan nama kluster kamu.  
> Kamu bisa ubah `--node-type` dan jumlah node sesuai kebutuhan.

---

### üîç Tambahan: Cek EKS Managed Node Group

Cek apakah node group sudah aktif via:

```bash
eksctl get nodegroup --cluster <CLUSTER_NAME>
```

Kalau tidak ada node group aktif, memang harus bikin baru dulu.

---

### üìå Setelah Node Aktif

Setelah kamu punya node yang `Ready` (tanpa SchedulingDisabled), lanjutkan:

1. Coba lagi apply manifest kamu:
   ```bash
   kubectl apply -f next/next-deployment.yml
   ```
2. Cek pod ALB Controller:
   ```bash
   kubectl get pods -n kube-system
   ```

Kalau `aws-load-balancer-controller` sudah `Running`, masalah Ingress kamu akan hilang ‚ú®

---

Kalau kamu mau dibantu nulis command eksctl lengkap (termasuk IAM Role/IAM OIDC setup kalau belum), tinggal bilang ya.